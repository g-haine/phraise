#!/usr/bin/env bash
set -euo pipefail

# Ce fichier contient les différentes fonctions utiles à plusieurs scripts à la fois

# Fonctions helper
log () { printf '[*] %s\n' "$*" >&2; return 0; }
warn () { printf '[!] %s\n' "$*" >&2; return 0; }
die () { printf '[X] %s\n' "$*" >&2; exit 1; }
out () { 
  if [[ -n "$*" ]]; then
    printf '%s\n' "$*"
  fi
  return 0
}

# Mail and API keys
if [ -f .env ]; then
  source .env
else
  die "Error: .env file is missing!"
fi

# Une fonction de slugify des titres
slugify () {
  local out
  if out=$(echo "${1:0:240}" |
          iconv -t ascii//TRANSLIT |
          tr '[:upper:]' '[:lower:]' |
          tr -cs 'a-z0-9' '-' |
          sed -E 's/^-+|-+$//g'); then
    out "$out"
  fi
  return 0
}

# Remplace les $ par les balises markdwon de mathjax
mathjaxify () {
  local out
  if out=$(echo "$1" |
          sed -e 's/$$/$/g' | # au cas où il y aurait des double $
          sed -e 's/{{/{[[:space:]]{/g' | # Pour éviter les conflits avec jekyll
          sed -e 's/}}/}[[:space:]]}/g' | # Pour éviter les conflits avec jekyll
          # Mathjax \( ... \) avec les escape nécessaires
          awk '{
              in_math = 0;
              for (i=1; i<=length($0); i++) {
                  c = substr($0, i, 1);
                  if (c == "$") {
                      if (in_math == 0) {
                          printf "\\\\( ";
                          in_math = 1;
                      } else {
                          printf " \\\\)";
                          in_math = 0;
                      }
                  } else {
                      printf "%s", c;
                  }
              }
              printf "\n";
          }'); then
    out "$out"
  fi
  return 0
}

# Safe curl
safe_curl () {
  local url="$1"
  shift
  local headers=()
  for h in "$@"; do
    headers+=(-H "$h")
  done
  if [[ ${#headers[@]} -gt 0 ]]; then
    response=$(curl -Ls --connect-timeout 5 --retry 3 --retry-delay 2 -w "%{http_code}" "${headers[@]}" -X GET "$url")
  else
    response=$(curl -Ls --connect-timeout 5 --retry 3 --retry-delay 2 -w "%{http_code}" -X GET "$url")
  fi
  local 
  local http=${request: -3}
  if [[ "$http" -eq 200 ]]; then
    local response=${request:: -3}
    out "$response"
  else
    out '{"status":"error"}'
  fi
  return 0
}

# Une fonctione qui récupère le bibtex, le formatte et le sauve dans un fichier
print_bib () {
  local doi="$1"
  local file="$2"
  local noBibtex="No BibTeX found!"
  local enc_doi=$(jq -rn --arg d "$doi" '$d|@uri')
  local doiBib="$(safe_curl "https://doi.org/$enc_doi" "Accept: application/x-bibtex;q=1.0")"
  if echo "$doiBib" | grep -q '{"status":"error"}'; then
    echo "$noBibtex" > "$file"
    return 0
  fi
  local bibtex
  if ! bibtex="$(
    printf '%s\n' "$doiBib" | sed -E '
      s/ @/@/g;
      s/,\s*}$/\n}/;                                   # pas de virgule avant }
      s/,\s*(series|pages|title)\b/,\n  \1/g;          # sauts de ligne jolis
      /^\s*title\s*=/ s/=\{/= {{/;                     # double accolades pour title
      /^\s*title\s*=/ s/},/}},/;
      /^\s*pages\s*=/ s/–/--/g;                        # en dash -> --
      /^[[:space:]]*month[[:space:]]*=/d;              # supprime month=
      /^[[:space:]]*url[[:space:]]*=/d                 # supprime url=
    '
  )"; then
    echo "$noBibtex" > "$file"
    return 0
  fi
  if ! printf '%s' "$bibtex" | grep -q '^@'; then
    echo "$noBibtex" > "$file"
    return 0
  fi
  local tmp
  tmp="$(mktemp)"
  printf '%s\n' "$bibtex" > "$tmp"
  mv -f -- "$tmp" "$file"
  return 0
}

# Appels aux API -Ls --connect-timeout 5 --retry 3 --retry-delay 2
fetch_metadata () {
  local doi="$1"
  local enc_doi=$(jq -rn --arg d "$doi" '$d|@uri')
  local url=""
  local headers=()
  case "$2" in
    crossref)
      url="https://api.crossref.org/works/$enc_doi"
      [[ -n "$MAIL" ]] && url+="?mailto=$MAIL"
      ;;
    scopus)
      url="https://api.elsevier.com/content/article/doi/$enc_doi"
      headers+=("Accept: application/json")
      headers+=("X-ELS-APIKey: ${SCOPUS_API_KEY:?SCOPUS_API_KEY not set}")
      ;;
    springer)
      url="https://api.springernature.com/meta/v2/json?q=doi:$enc_doi&api_key=${SPRINGER_API_KEY:?SPRINGER_API_KEY not set}"
      ;;
    ieee)
      url="https://ieeexploreapi.ieee.org/api/v1/articles/doi/$enc_doi?apikey=${IEEE_API_KEY:?IEEE_API_KEY not set}&format=json"
      ;;
    *)
      die "Unknown API!"
      ;;
  esac
  local response
  response=$(safe_curl "$url" "${headers[@]}")
  out "$response"
  return 0
}

# Appel à l'API de crossref
fetch_metadata_crossref () {
  local response=$(fetch_metadata "$1" "crossref")
  out "$response"
  return 0
}

# Appel à l'API de scopus
fetch_metadata_scopus () {
  local response=$(fetch_metadata "$1" "scopus")
  out "$response"
  return 0
}

# Appel à l'API de Springer
fetch_metadata_springer () {
  local response=$(fetch_metadata "$1" "springer")
  out "$response"
  return 0
}

# Appel à l'API IEEE Xplore
fetch_metadata_ieee () {
  local response=$(fetch_metadata "$1" "ieee")
  out "$response"
  return 0
}

# Lis un JSON
read_json () {
  if ! jq -e . >/dev/null 2>&1 <<<"$1"; then
    return 0
  fi
  local out
  if out=$(jq -r "$2" <<<"$1"); then
    out "$out"
  fi
  return 0
}

# Abstract via scopus
abstract_from_scopus () {
  out $(read_json "$1" '
      (.["full-text-retrieval-response"].coredata."dc:description"
        // "")
      | tostring
      | gsub("<[^>]+>"; "")
      | gsub("\\s+"; " ")
      | gsub("^\\s+|\\s+$"; "")
  ')
  return 0
}

# Keywords via scopus
keywords_from_scopus () {
  out $(read_json "$1" '
      def norm(s):
        s | tostring
          | ascii_downcase
          | gsub("^\\s+|\\s+$"; "");

      def from_subjects:
        (
          .["full-text-retrieval-response"].coredata."dcterms:subject"
          // []
        )
        | map(.["$"] // empty)
        | map(norm(.))
        | map(select(length > 0));

      def from_authkeywords:
        (
          .["full-text-retrieval-response"].coredata.authkeywords
          // ""
        )
        | tostring
        | if . == "" or . == "null" then [] else
            ( split("[|;,]"; "x") | map(norm(.)) )
          end
        | map(select(length > 0));

        ( (from_subjects + from_authkeywords) | unique ) | join(", ")
      ')
  return 0
}

# event via scopus : super pour le nom de la conf IFAC
event_from_scopus() {
  out $(read_json "$1" '.["full-text-retrieval-response"].coredata."prism:issueName" // ""')
  return 0
}

# Abstract via Springer
abstract_from_springer () {
  out $(read_json "$1" '
        (
          .records[0].abstract
          // ""
        )
        | tostring
        | gsub("<[^>]+>"; "")
        | gsub("\\s+"; " ")
        | gsub("^\\s+|\\s+$"; "")
      ')
  return 0
}

# Keywords via Springer
keywords_from_springer () {
  out $(read_json "$1" '
      def norm(s):
        s | tostring
          | ascii_downcase
          | gsub("^\\s+|\\s+$"; "");

      if (.records | type == "array") and (.records | length > 0) then
        (
          .records[0].keyword
          | if type == "array" then
              ( map(norm(.)) )
            elif type == "string" then
              ( splits("[,;|]") | map(norm(.)) )
            else
              []
            end
          | map(select(length > 0))
          | unique
          | join(", ")
        )
      else "" end
      ')
  return 0
}

# event via springer : super pour le nom de la conf également, type GSI
event_from_springer() {
  out $(read_json "$1" '
        .records[0].conferenceInfo? // []         # tableau ou []
        | .[]                                     # chaque entrée
        | .confSeriesName? // empty               # valeur ou ""
      ')
  return 0
}

# Abstract via IEEE
abstract_from_ieee () {
  out $(read_json "$1" '
        (
          .articles[0].abstract
          // ""
        )
        | tostring
        | gsub("<[^>]+>"; "")
        | gsub("\\s+"; " ")
        | gsub("^\\s+|\\s+$"; "")
      ')
  return 0
}

# Keywords via IEEE (fusionne author_terms + ieee_terms + index_terms le cas échéant)
keywords_from_ieee () {
  out $(read_json "$1" '
        def norm(s):
          s | tostring
            | ascii_downcase
            | gsub("^\\s+|\\s+$"; "");

        def arr(x):
          if x|type=="array" then x else [] end;

        if (.articles|type=="array") and (.articles|length>0) then
          (
            ( arr(.articles[0].author_terms)
            + arr(.articles[0].ieee_terms)
            + ( if .articles[0].index_terms|type=="object" then
                  ( arr(.articles[0].index_terms.author_terms)
                  + arr(.articles[0].index_terms.ieee_terms) )
                else [] end )
            )
            | map(norm(.))
            | map(select(length>0))
            | unique
            | join(", ")
          )
        else "" end
      ')
  return 0
}

# Event (nom de la conférence) via IEEE
event_from_ieee() {
  out $(read_json "$1" '
        if (.articles|type=="array" and (.articles|length>0)) then
          (.articles[0]
            | ( .content_type|tostring|ascii_downcase ) as $t
            | if $t == "conferences" then (.publication_title // "") else "" end
          )
        else "" end
      ')
  return 0
}

abstract_from_mendeley () {
  local doi="$1"
  json=$(safe_curl "https://api.mendeley.com/catalog?doi=$doi&view=all" "accept: application/vnd.mendeley-document.1+json" "Authorization: Bearer $MENDELEY_API_KEY")
  
  # 1) récupère le lien de la page Mendeley
  local link=$(read_json "$json" '[0].link?')
  if [ -z "$link" ] || [ "$link" = "null" ]; then
    jq -n --arg doi "$doi" '{abstract:null, url:null, method:"none", source:"mendeley_link_scrape", error:"no_link_from_mendeley", doi:$doi}'
    return 1
  fi

  # 2) Scrape la page ciblée (sans scraping agressif ; simple GET + extraction de balises/meta)
  python3 - "$link" <<'PY'
import sys, re, json, html
from bs4 import BeautifulSoup
import requests

url = sys.argv[1]
UA = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari (abstract-fallback)"

def clean_text(t: str) -> str:
    t = html.unescape(t or "")
    t = re.sub(r'\s+', ' ', t).strip()
    return t

try:
    r = requests.get(url, headers={"User-Agent": UA, "Accept":"text/html,*/*;q=0.8"}, timeout=20, allow_redirects=True)
    r.raise_for_status()
except Exception as e:
    print(json.dumps({"abstract": None, "url": url, "method":"http_error", "source":"mendeley_link_scrape", "error": str(e)}))
    sys.exit(0)

soup = BeautifulSoup(r.text, "lxml")

candidates = []

# === 1) Carte "Abstract" stricte ===
# on trouve le titre "Abstract" puis on remonte au conteneur .card correspondant
title = soup.find(lambda tag: tag.name in ("h2","h3","h4","h5")
                  and tag.get("data-name") in ("abstract-title",)
                  or (tag.name in ("h2","h3","h4","h5")
                      and re.fullmatch(r'\s*abstract\s*', tag.get_text(strip=True), flags=re.I)))
container = None
if title:
    # remonte au div/section parent qui porte la classe 'card' (extrait fourni)
    cur = title
    while cur and cur.name not in ("div","section","article"):
        cur = cur.parent
    while cur and "card" not in (cur.get("class") or []):
        # certaines libs génèrent "card__Container-xxxx"
        if any(isinstance(c, str) and c.startswith("card__") for c in (cur.get("class") or [])):
            break
        cur = cur.parent
    container = cur or title.parent

abstract_text = None
if container:
    # cible exact: p[data-name="content"] puis concatène tous les spans internes
    target = container.select_one('p[data-name="content"]')
    if target:
        spans = target.find_all("span")
        if spans:
            pieces = [s.get_text(" ", strip=True) for s in spans if s.get_text(strip=True)]
            abstract_text = " ".join(pieces).strip()
        else:
            abstract_text = target.get_text(" ", strip=True).strip()
        abstract_text = clean_text(abstract_text)

# === 2) Fallback: autres sélecteurs classiques, mais seulement si rien trouvé ===
if not abstract_text:
    for sel in [
        '#Abs1-content',                 # Springer
        'section.Abstract', 'div.Abstract',
        'div#abstract', 'section#abstract',
        'div.article__abstract', 'article .abstract',
    ]:
        el = soup.select_one(sel)
        if el:
            txt = clean_text(el.get_text(" ", strip=True))
            if len(txt.split()) > 30:  # évite les teasers
                abstract_text = txt
                break

# === 3) Dernier recours: meta/og, mais filtre les teasers courts/avec "..." ===
def first_meta(*names):
    for n in names:
        el = soup.find("meta", attrs={"name": n}) or soup.find("meta", attrs={"property": n})
        if el and el.get("content"):
            return el["content"].strip()
    return None

if not abstract_text:
    for key in ["citation_abstract", "dcterms.abstract", "DC.Description", "dc.Description",
                "og:description", "description"]:
        v = first_meta(key)
        v = clean_text(v or "")
        # rejette si trop court ou si ça finit par "..." (teaser)
        if v and len(v.split()) >= 40 and not v.endswith("..."):
            abstract_text = v
            break

# Nettoyages finaux
if abstract_text:
    abstract_text = re.sub(r'^(abstract|résumé|summary|zusammenfassung)\s*[:–—-]\s*', '', abstract_text, flags=re.I)
    # si le texte est encore suspect (teaser), on le rejette
    if len(abstract_text.split()) < 40:
        abstract_text = None

method = "card:abstract-title+content" if container and abstract_text else ("css_fallback" if abstract_text else "not_found")

print(json.dumps({
    "abstract": abstract_text,
    "url": r.url,
    "method": method,
    "source": "mendeley_link_scrape"
}, ensure_ascii=False))
PY
}

# Récupération des abstracts (OpenAlex, Semantic Scholar)
fetch_abstract_complement () {
    local doi="$1"
    local abstract="Not available"
    local candidates=()

    # 1. Semantic Scholar
    local semantics_response
    semantics_response=$(safe_curl "https://api.semanticscholar.org/graph/v1/paper/DOI:$doi?fields=abstract")
    if echo "$semantics_response" | jq empty 2>/dev/null; then
        local semantics_abstract
        semantics_abstract=$(echo "$semantics_response" | jq -r '.abstract // ""')
        [ -n "$semantics_abstract" ] && [ "$semantics_abstract" != "null" ] && candidates+=("$semantics_abstract")
    fi
    
    # 2. Mendeley
    local mendeley_response
    mendeley_response=$(abstract_from_mendeley "$doi")
    if echo "$mendeley_response" | jq empty 2>/dev/null; then
        local mendeley_abstract
        mendeley_abstract=$(echo "$mendeley_response" | jq -r '.abstract // ""')
        [ -n "$mendeley_abstract" ] && [ "$mendeley_abstract" != "null" ] && candidates+=("$mendeley_abstract")
    fi

    # 3. Sélection du candidat le plus long (après trim)
    for candidate in "${candidates[@]}"; do
        local trimmed
        trimmed="$(echo "$candidate" | sed -E 's/^[[:space:]]+|[[:space:]]+$//g')"
        if [ -n "$trimmed" ] && [ "$trimmed" != "null" ]; then
            if [ "${#trimmed}" -gt "${#abstract}" ] || [ "$abstract" = "Not available" ]; then
                abstract="$trimmed"
            fi
        fi
    done

    out "$abstract"
    return 0
}

# Récupère la citation formattée d'après un DOI
get_citation () {
    local doi="$1"
    local citation
    if citation=$(safe_curl "https://citation.doi.org/format?doi=$doi&style=springer-basic-author-date-no-et-al-with-issue&lang=en-US"); then
      local out
      if out=$(echo "$citation" | tail -n 1 | sed 's/^1.[[:space:]]//g' | sed 's/.$//g'); then
        out "$out"
      fi
    fi
    return 0
}

# Ajoute un zéro si le mois ou le jour est entre 1 et 9
pad_zero () {
    printf "%02d" "$1"
    return 0
}

