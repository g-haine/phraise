#!/usr/bin/env bash

# Ce fichier contient les différentes fonctions utiles à plusieurs scripts à la fois

# Mail, Scopus key and Springer key
if [ -f .env ]; then
    source .env
else
    echo "Erreur : fichier .env introuvable !" >&2
    exit 1
fi

# Une fonction de slugify des titres
slugify () {
    echo "${1:0:240}" |
    iconv -t ascii//TRANSLIT |
    tr '[:upper:]' '[:lower:]' |
    tr -cs 'a-z0-9' '-' |
    sed -E 's/^-+|-+$//g'
}

# Une fonctione qui récupère le bibtex et le formatte
print_bib () {
    local doi=$1
    local bibfile=$2
    doiBib="$(curl -s -LH "Accept: application/x-bibtex;q=1.0" https://doi.org/$doi
)"
    bibtex="$(echo "$doiBib" \
        | sed -e 's/ @/@/g' \
        | sed -e 's/},/},\n /g' \
        | sed -e 's/, series/,\n  series/g' \
        | sed -e 's/, pages/,\n  pages/g' \
        | sed -e 's/, title/,\n  title/g' \
        | sed -e '/title/s/={/={{/g' \
        | sed -e '/title/s/},/}},/g' \
        | sed -e 's/ }/\n}/g' \
        | sed -e 's/, }/\n}/g' \
        | sed -e '/pages/s/–/--/g' \
        | sed -e '/month/d' \
        | sed -e '/url/d' \
        | tac | sed -e '2 s/,//g' | tac)"
    echo "$bibtex" > $bibfile
}

# Remplace les $ par les balises markdwon de mathjax
mathjaxify () {
    text=$(echo "$1" | sed -e 's/$$/$/g') # au cas où il y aurait des double $
    text=$(echo "$text" | sed -e 's/{{/{[[:space:]]{/g') # Pour éviter les conflits avec jekyll
    text=$(echo "$text" | sed -e 's/}}/}[[:space:]]}/g') # Pour éviter les conflits avec jekyll
    # Mathjax \( ... \) avec les escape nécessaires
    text=$(echo "$text" \
    | awk '{
        in_math = 0;
        for (i=1; i<=length($0); i++) {
            c = substr($0, i, 1);
            if (c == "$") {
                if (in_math == 0) {
                    printf "\\\\( ";
                    in_math = 1;
                } else {
                    printf " \\\\)";
                    in_math = 0;
                }
            } else {
                printf "%s", c;
            }
        }
        printf "\n";
    }')
    
    [ -n "$text" ] && [ "$text" != "null" ] && printf '%s\n' "$text"
}

# Appel à l'API de crossref
fetch_metadata_crossref () {
    local doi=$1
    request=$(curl -s -w "%{http_code}" "https://api.crossref.org/works/$doi?mailto=$MAIL")
    http=${request: -3}
    response=${request:: -3}
    if [[ "$http" -eq 200 ]]; then
        echo "$response"
    else
        echo '{"status":"error"}'
    fi
}

# Appel à l'API de scopus
fetch_metadata_scopus () {
    local doi=$1
    local enc_doi
    enc_doi=$(jq -rn --arg d "$doi" '$d|@uri')
    echo $(curl -s -X GET "https://api.elsevier.com/content/article/doi/$enc_doi" -H "Accept: application/json" -H "X-ELS-APIKey: $SCOPUS_API_KEY")
}

# Abstract via scopus
abstract_from_scopus () {
    local json=$1
    if ! jq -e . >/dev/null 2>&1 <<<"$json"; then
        return 0
    fi

    local abs
    abs=$(jq -r '
        (
          .["full-text-retrieval-response"].coredata."dc:description"
          // ""
        )
        | tostring
        | gsub("<[^>]+>"; "")
        | gsub("\\s+"; " ")
        | gsub("^\\s+|\\s+$"; "")
    ' <<<"$json") || :

    [[ -n $abs ]] && printf '%s\n' "$abs"
}

# Keywords via scopus
keywords_from_scopus () {
    local json=$1
    jq -e empty <<<"$json" >/dev/null 2>&1 || return 0

    local out
    out=$(jq -r '
        def norm(s):
          s | tostring
            | ascii_downcase
            | gsub("^\\s+|\\s+$"; "");

        def from_subjects:
          (
            .["full-text-retrieval-response"].coredata."dcterms:subject"
            // []
          )
          | map(.["$"] // empty)
          | map(norm(.))
          | map(select(length > 0));

        def from_authkeywords:
          (
            .["full-text-retrieval-response"].coredata.authkeywords
            // ""
          )
          | tostring
          | if . == "" or . == "null" then [] else
              ( split("[|;,]"; "x") | map(norm(.)) )
            end
          | map(select(length > 0));

        ( (from_subjects + from_authkeywords) | unique ) | join(", ")
    ' <<<"$json") || :

    [[ -n $out ]] && printf '%s\n' "$out"
}

# event via scopus : super pour le nom de la conf IFAC
event_from_scopus() {
    local json=$1

    # 1) Valider le JSON sans pipeline
    jq -e empty <<<"$json" >/dev/null 2>&1 || return 0

    # 2) Extraire la valeur sans faire tomber le script si jq échoue
    local scopus_nameIssue
    scopus_nameIssue=$(jq -r '.["full-text-retrieval-response"].coredata."prism:issueName" // ""' <<<"$json") || :

    # 3) Afficher si non vide
    [[ -n $scopus_nameIssue ]] && printf '%s\n' "$scopus_nameIssue"
}


# Appel à l'API de Springer
fetch_metadata_springer () {
    local doi=$1
    local enc_doi
    enc_doi=$(jq -rn --arg d "$doi" '$d|@uri')
    echo $(curl -s -X GET "https://api.springernature.com/meta/v2/json?q=doi:$enc_doi&api_key=$SPRINGER_API_KEY")
}

# Abstract via Springer
abstract_from_springer () {
    local json=$1
    if ! jq -e . >/dev/null 2>&1 <<<"$json"; then
        return 0
    fi

    local abs
    abs=$(jq -r '
        (
          .records[0].abstract
          // ""
        )
        | tostring
        | gsub("<[^>]+>"; "")
        | gsub("\\s+"; " ")
        | gsub("^\\s+|\\s+$"; "")
    ' <<<"$json") || :

    [[ -n $abs ]] && printf '%s\n' "$abs"
}

# Keywords via Springer
keywords_from_springer () {
    local json=$1
    if ! jq -e . >/dev/null 2>&1 <<<"$json"; then
        return 0
    fi

    local out
    if out=$(jq -r '
        def norm(s):
          s | tostring
            | ascii_downcase
            | gsub("^\\s+|\\s+$"; "");

        if (.records | type == "array") and (.records | length > 0) then
          (
            .records[0].keyword
            | if type == "array" then
                ( map(norm(.)) )
              elif type == "string" then
                ( splits("[,;|]") | map(norm(.)) )
              else
                []
              end
            | map(select(length > 0))
            | unique
            | join(", ")
          )
        else "" end
    ' <<<"$json"); then
        if [[ -n $out ]]; then
            printf '%s\n' "$out"
        fi
    fi
    return 0
}

# event via springer : super pour le nom de la conf également, type GSI
event_from_springer() {
    local json=$1

    # 1) Valider le JSON (sans pipeline)
    jq -e empty >/dev/null 2>&1 <<<"$json" || return 0

    # 2) Extraire (tolérant aux champs manquants)
    #    -> imprime 0..N lignes (une par confSeriesName trouvé)
    local out
    out=$(jq -r '
        .records[0].conferenceInfo? // []         # tableau ou []
        | .[]                                     # chaque entrée
        | .confSeriesName? // empty               # valeur ou ""
    ' <<<"$json") || :                            # ne jamais tuer le script

    [[ -n $out ]] && printf '%s\n' "$out"
}

# Appel à l'API IEEE Xplore (DOI d'abord, repli sur /search)
fetch_metadata_ieee () {
    local doi=$1
    local enc_doi
    enc_doi=$(jq -rn --arg d "$doi" '$d|@uri')
    echo $(curl -s "https://ieeexploreapi.ieee.org/api/v1/articles/doi/${enc_doi}?apikey=${IEEE_API_KEY}&format=json")
}

# Abstract via IEEE
abstract_from_ieee () {
    local json=$1
    if ! jq -e . >/dev/null 2>&1 <<<"$json"; then
        return 0
    fi

    # L’abstract est dans articles[0].abstract
    local abs
    abs=$(jq -r '
        (
          .articles[0].abstract
          // ""
        )
        | tostring
        | gsub("<[^>]+>"; "")
        | gsub("\\s+"; " ")
        | gsub("^\\s+|\\s+$"; "")
    ' <<<"$json") || :

    [ -n "$abs" ] && [ "$abs" != "null" ] &&printf '%s\n' "$abs"
}

# Keywords via IEEE (fusionne author_terms + ieee_terms + index_terms le cas échéant)
keywords_from_ieee () {
    local json=$1
    if ! jq -e . >/dev/null 2>&1 <<<"$json"; then
        return 0
    fi

    # Les mots-clés peuvent apparaître sous:
    # - articles[0].author_terms (array de strings)
    # - articles[0].ieee_terms   (array de strings, thésaurus contrôlé)
    # - articles[0].index_terms  (objet possédant possiblement author_terms/ieee_terms)
    local out
    out=$(jq -r '
        def norm(s):
          s | tostring
            | ascii_downcase
            | gsub("^\\s+|\\s+$"; "");

        def arr(x):
          if x|type=="array" then x else [] end;

        if (.articles|type=="array") and (.articles|length>0) then
          (
            ( arr(.articles[0].author_terms)
            + arr(.articles[0].ieee_terms)
            + ( if .articles[0].index_terms|type=="object" then
                  ( arr(.articles[0].index_terms.author_terms)
                  + arr(.articles[0].index_terms.ieee_terms) )
                else [] end )
            )
            | map(norm(.))
            | map(select(length>0))
            | unique
            | join(", ")
          )
        else "" end
    ' <<<"$json") || :

    [ -n "$out" ] && [ "$out" != "null" ] && printf '%s\n' "$out"
}

# Event (nom de la conférence) via IEEE
# Pour les contenus de type "Conferences", le nom de l’événement est généralement dans publication_title
# D’autres infos utiles existent: conference_dates, conference_location
event_from_ieee() {
    local json=$1

    # 1) Valider le JSON
    jq -e empty >/dev/null 2>&1 <<<"$json" || return 0

    # 2) Extraire le nom si c'est bien un "conferences"
    local name
    if name=$(jq -r '
        if (.articles|type=="array" and (.articles|length>0)) then
          (.articles[0]
            | ( .content_type|tostring|ascii_downcase ) as $t
            | if $t == "conferences" then (.publication_title // "") else "" end
          )
        else "" end
    ' <<<"$json"); then
        [[ -n $name && $name != "null" ]] && printf '%s\n' "$name"
    fi
}

abstract_from_mendeley_fallback () {
  local DOI="$1"
  local TOKEN="$MENDELEY_API_KEY"

  if [ -z "$DOI" ]; then
    echo 'Usage: abstract_from_mendeley_fallback "<DOI>"' >&2
    return 2
  fi

  # 1) Récupère le lien depuis Mendeley Catalog
  JSON=$(
    curl -X "GET" \
      "https://api.mendeley.com/catalog?doi=$DOI&view=all" \
      -H "accept: application/vnd.mendeley-document.1+json" \
      -H "Authorization: Bearer $TOKEN"
  )

  local LINK
  LINK=$(jq -r '[0].link?' <<<"$JSON")

  if [ -z "$LINK" ] || [ "$LINK" = "null" ]; then
    jq -n --arg doi "$DOI" '{abstract:null, url:null, method:"none", source:"mendeley_link_scrape", error:"no_link_from_mendeley", doi:$doi}'
    return 1
  fi

  # 2) Scrape la page ciblée (sans scraping agressif ; simple GET + extraction de balises/meta)
  python3 - "$LINK" <<'PY'
import sys, re, json, html
from bs4 import BeautifulSoup
import requests

url = sys.argv[1]
UA = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari (abstract-fallback)"

def clean_text(t: str) -> str:
    t = html.unescape(t or "")
    t = re.sub(r'\s+', ' ', t).strip()
    return t

try:
    r = requests.get(url, headers={"User-Agent": UA, "Accept":"text/html,*/*;q=0.8"}, timeout=20, allow_redirects=True)
    r.raise_for_status()
except Exception as e:
    print(json.dumps({"abstract": None, "url": url, "method":"http_error", "source":"mendeley_link_scrape", "error": str(e)}))
    sys.exit(0)

soup = BeautifulSoup(r.text, "lxml")

candidates = []

# === 1) Carte "Abstract" stricte ===
# on trouve le titre "Abstract" puis on remonte au conteneur .card correspondant
title = soup.find(lambda tag: tag.name in ("h2","h3","h4","h5")
                  and tag.get("data-name") in ("abstract-title",)
                  or (tag.name in ("h2","h3","h4","h5")
                      and re.fullmatch(r'\s*abstract\s*', tag.get_text(strip=True), flags=re.I)))
container = None
if title:
    # remonte au div/section parent qui porte la classe 'card' (extrait fourni)
    cur = title
    while cur and cur.name not in ("div","section","article"):
        cur = cur.parent
    while cur and "card" not in (cur.get("class") or []):
        # certaines libs génèrent "card__Container-xxxx"
        if any(isinstance(c, str) and c.startswith("card__") for c in (cur.get("class") or [])):
            break
        cur = cur.parent
    container = cur or title.parent

abstract_text = None
if container:
    # cible exact: p[data-name="content"] puis concatène tous les spans internes
    target = container.select_one('p[data-name="content"]')
    if target:
        spans = target.find_all("span")
        if spans:
            pieces = [s.get_text(" ", strip=True) for s in spans if s.get_text(strip=True)]
            abstract_text = " ".join(pieces).strip()
        else:
            abstract_text = target.get_text(" ", strip=True).strip()
        abstract_text = clean_text(abstract_text)

# === 2) Fallback: autres sélecteurs classiques, mais seulement si rien trouvé ===
if not abstract_text:
    for sel in [
        '#Abs1-content',                 # Springer
        'section.Abstract', 'div.Abstract',
        'div#abstract', 'section#abstract',
        'div.article__abstract', 'article .abstract',
    ]:
        el = soup.select_one(sel)
        if el:
            txt = clean_text(el.get_text(" ", strip=True))
            if len(txt.split()) > 30:  # évite les teasers
                abstract_text = txt
                break

# === 3) Dernier recours: meta/og, mais filtre les teasers courts/avec "..." ===
def first_meta(*names):
    for n in names:
        el = soup.find("meta", attrs={"name": n}) or soup.find("meta", attrs={"property": n})
        if el and el.get("content"):
            return el["content"].strip()
    return None

if not abstract_text:
    for key in ["citation_abstract", "dcterms.abstract", "DC.Description", "dc.Description",
                "og:description", "description"]:
        v = first_meta(key)
        v = clean_text(v or "")
        # rejette si trop court ou si ça finit par "..." (teaser)
        if v and len(v.split()) >= 40 and not v.endswith("..."):
            abstract_text = v
            break

# Nettoyages finaux
if abstract_text:
    abstract_text = re.sub(r'^(abstract|résumé|summary|zusammenfassung)\s*[:–—-]\s*', '', abstract_text, flags=re.I)
    # si le texte est encore suspect (teaser), on le rejette
    if len(abstract_text.split()) < 40:
        abstract_text = None

method = "card:abstract-title+content" if container and abstract_text else ("css_fallback" if abstract_text else "not_found")

print(json.dumps({
    "abstract": abstract_text,
    "url": r.url,
    "method": method,
    "source": "mendeley_link_scrape"
}, ensure_ascii=False))
PY
}

# Récupération des abstracts (OpenAlex, Semantic Scholar)
fetch_abstract_complement () {
    local doi=$1
    local abstract="Not available"
    local candidates=()

    # Fonction de sécurité pour les appels API avec suivi des redirections
    safe_curl () {
        curl -Ls --connect-timeout 5 --retry 3 --retry-delay 2 "$1" || echo ""
    }

    # 1. Semantic Scholar
    local semantics_response
    semantics_response=$(safe_curl "https://api.semanticscholar.org/graph/v1/paper/DOI:$doi?fields=abstract")
    if echo "$semantics_response" | jq empty 2>/dev/null; then
        local semantics_abstract
        semantics_abstract=$(echo "$semantics_response" | jq -r '.abstract // ""')
        [ -n "$semantics_abstract" ] && [ "$semantics_abstract" != "null" ] && candidates+=("$semantics_abstract")
    fi
    
    # 2. Mendeley
    local mendeley_response
    mendeley_response=$(abstract_from_mendeley_fallback "$doi")
    if echo "$mendeley_response" | jq empty 2>/dev/null; then
        local mendeley_abstract
        mendeley_abstract=$(echo "$mendeley_response" | jq -r '.abstract // ""')
        [ -n "$mendeley_abstract" ] && [ "$mendeley_abstract" != "null" ] && candidates+=("$mendeley_abstract")
    fi

    # 3. Sélection du candidat le plus long (après trim)
    for candidate in "${candidates[@]}"; do
        local trimmed
        trimmed="$(echo "$candidate" | sed -E 's/^[[:space:]]+|[[:space:]]+$//g')"
        if [ -n "$trimmed" ] && [ "$trimmed" != "null" ]; then
            if [ "${#trimmed}" -gt "${#abstract}" ] || [ "$abstract" = "Not available" ]; then
                abstract="$trimmed"
            fi
        fi
    done

    [ -n "$abstract" ] && [ "$abstract" != "null" ] &&printf '%s\n' "$abstract"
}

# Récupère la citation formattée d'après un DOI
get_citation () {
    local doi=$1
    citation=$(curl -s https://citation.doi.org/format?doi=$doi&style=springer-basic-author-date-no-et-al-with-issue&lang=en-US | tail -n 1)
    echo "$citation" | sed 's/^1.[[:space:]]//g' | sed 's/.$//g'
}

# Ajoute un zéro si le mois ou le jour est entre 1 et 9
pad_zero () {
    printf "%02d" "$1"
}

